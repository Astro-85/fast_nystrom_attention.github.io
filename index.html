<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Artifacts and Attention Sinks Project Page</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            margin: 0;
            background-color: #fdfdfd;
        }
        .container {
            max-width: 900px;
            margin: 2em auto;
            padding: 0 20px;
        }
        h1, h2, h3 {
            font-weight: 500;
            line-height: 1.2;
            color: #111;
        }
        h1 {
            font-size: 2.5em;
            text-align: center;
            margin-bottom: 0.2em;
        }
        .authors {
            text-align: center;
            font-size: 1.1em;
            margin-bottom: 0.5em;
        }
        .affiliation {
            text-align: center;
            font-size: 1em;
            color: #555;
            margin-bottom: 2em;
        }
        .links {
            text-align: center;
            margin-bottom: 3em;
        }
        .links a {
            display: inline-block;
            text-decoration: none;
            background-color: #007bff;
            color: white;
            padding: 10px 20px;
            border-radius: 5px;
            margin: 5px;
            font-weight: bold;
            transition: background-color 0.2s;
        }
        .links a:hover {
            background-color: #0056b3;
        }
        figure {
            margin: 2em 0;
            text-align: center;
        }
        img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        figcaption {
            margin-top: 0.8em;
            font-style: italic;
            color: #666;
            font-size: 0.9em;
        }
        p, li {
            font-size: 1.05em;
        }
        .section {
            margin-bottom: 3em;
        }
        .highlight {
            background-color: #e7f3ff;
            border-left: 4px solid #007bff;
            padding: 1em 1.5em;
            margin: 1.5em 0;
        }
        pre {
            background-color: #f4f4f4;
            border: 1px solid #ddd;
            padding: 1em;
            overflow-x: auto;
            border-radius: 4px;
        }
        code {
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
        }
    </style>
</head>
<body>

    <div class="container">

        <header>
            <h1>Artifacts and Attention Sinks: Structured Approximations for Efficient Vision Transformers</h1>
            <p class="authors">
                Andrew Lu, Wentinn Liao, Liuhui Wang, Huzheng Yang, Jianbo Shi
            </p>
            <p class="affiliation">
                University of Pennsylvania
            </p>
            <div class="links">
                <a href="#">[ Paper ]</a>
                <a href="#">[ Code ]</a>
                <a href="#citation">[ BibTeX ]</a>
            </div>
        </header>

        <main>
            <figure>
                <img src="./static/images/dino_clip_artifact_masking.png" alt="Visualization of sink token masking">
                <figcaption>
                    <b>Figure 1:</b> Visualizations of sink token masks applied to features in CLIP and DINOv2. Masking these tokens, which act as attention sinks, helps to denoise features and emphasize regions of interest without any additional training.
                </figcaption>
            </figure>

            <section id="abstract" class="section">
                <h2>Abstract</h2>
                <p>
                    Vision transformers have emerged as a powerful tool across a wide range of applications, yet their inner workings remain only partially understood. In this work, we examine the phenomenon of massive tokens—tokens with exceptionally high activation norms that act as attention sinks—and artifact tokens that emerge as a byproduct during inference. Our analysis reveals that these tokens mutually suppress one another through the attention mechanism, playing a critical role in regulating information flow within the network. Leveraging these insights, we introduce <strong>Fast Nyström Attention (FNA)</strong>, a training-free method that approximates self-attention in linear time and space by exploiting the structured patterns formed by massive and artifact tokens. Additionally, we propose a masking strategy to mitigate noise from these tokens, yielding modest performance gains at virtually no cost. We evaluate our approach on popular pretrained vision backbones and demonstrate competitive performance on retrieval, classification, segmentation, and visual question answering (VQA), all while reducing computational overhead.
                </p>
            </section>

            <section id="phenomenon" class="section">
                <h2>The Phenomenon: Massive Tokens and Attention Sinks</h2>
                <p>
                    In large vision transformers like CLIP and DINOv2, we observe that a small subset of tokens develop exceptionally high activation norms in the middle-to-late layers. These <strong>"massive tokens"</strong> act as <strong>"attention sinks,"</strong> attracting a disproportionate amount of attention from all other tokens in the sequence. This creates a highly structured but low-rank attention pattern.
                </p>
                <p>
                    Furthermore, we identify <strong>"artifact tokens,"</strong> a dormant set of tokens that can become massive if the original massive tokens are removed. This reveals a built-in redundancy mechanism where the model can dynamically assign the role of an attention sink. While critical for information flow, these sink tokens can also introduce noise and hinder the model's performance on downstream tasks.
                </p>
                <figure>
                    <img src="./static/images/attention_iterative_removal_padded.pdf" alt="Attention matrix visualization">
                    <figcaption>
                        <b>Figure 2:</b> Attention matrices from CLIP ViT L-14. In later layers (11-13), massive tokens (MA) emerge and become attention sinks, attracting a large proportion of attention from all other tokens (CLS, Artifact, and Normal).
                    </figcaption>
                </figure>
            </section>
            
            <section id="fna" class="section">
                <h2>Our Solution: Fast Nyström Attention (FNA)</h2>
                <div class="highlight">
                    <p>
                        <strong>Fast Nyström Attention (FNA)</strong> is a training-free, drop-in replacement for standard self-attention that achieves linear time and space complexity by exploiting the structure of attention sinks.
                    </p>
                </div>
                <p>
                    The emergence of attention sinks creates a low-rank structure in the attention matrix. This suggests that the full quadratic attention computation is redundant. FNA leverages this by approximating the attention matrix using the Nyström method. Instead of random sampling, we use a more structured approach:
                </p>
                <ul>
                    <li>We identify a small set of "landmark" tokens to form the basis of our approximation.</li>
                    <li>We use <strong>Farthest Point Sampling (FPS)</strong> to select these landmarks. FPS naturally picks out statistical outliers, which include the massive and artifact tokens, without needing to explicitly detect them.</li>
                    <li>By guaranteeing the inclusion of the [CLS] token, we preserve global context while capturing the most influential local interactions dominated by sink tokens.</li>
                </ul>
                <p>
                    This approach allows FNA to significantly reduce computational and memory overhead, making vision transformers more efficient for long sequences, without requiring any model retraining.
                </p>
                <figure>
                    <img src="./static/images/fp16_attention_timing_plot.pdf" alt="FNA performance comparison">
                    <figcaption>
                        <b>Figure 3:</b> FNA outperforms other linear attention methods in inference speed and scales much more favorably than standard quadratic attention (e.g., FlashAttention) for long sequences.
                    </figcaption>
                </figure>
                <figure>
                    <img src="./static/images/llava_vqa_bert_f1.pdf" alt="FNA performance comparison">
                    <figcaption>
                        <b>Figure 4:</b> BERTScore and generation speed on COCO VQA for different configurations of FNA applied to LLaVA-NeXT-7B. Each color represents a LLaVA model with FNA applied on the causal attention to image tokens at the specified span of layers. Spot size represent FNA sample sizes ranging from 16 to 512 tokens out of ~2500 image tokens. .
                    </figcaption>
                </figure>
            </section>
            
            <section id="masking" class="section">
                <h2>Performance Gains Through Strategic Masking</h2>
                <p>
                    Beyond improving efficiency, we can also boost model performance by addressing the noise introduced by sink tokens. In the final layers, these tokens can "sink" attention away from more meaningful image features, hindering the [CLS] token's ability to aggregate a complete global representation.
                </p>
                <p>
                    We propose a simple, training-free masking strategy: after the sink tokens have formed, we detect them and replace their features with those of nearby normal tokens in the final layers. This "denoising" step rebalances the attention distribution, allowing the model to focus on more informative parts of the image. As shown in our experiments, this leads to consistent performance improvements across retrieval, classification, and segmentation tasks.
                </p>
                 <figure>
                    <!-- FIGURE PLACEHOLDER: Insert Table 5 results from your paper here -->
                    <img src="https://via.placeholder.com/800x200.png?text=Table+5:+Performance+Gains+from+Masking" alt="Table of results from masking">
                    <figcaption>
                        <b>Table 1 Results:</b> Masking sink tokens in the final layers of CLIP ViT L-14 consistently improves zero-shot retrieval performance on COCO and Flickr30k.
                    </figcaption>
                </figure>
            </section>


            <section id="citation" class="section">
                <h2>Citation</h2>
                <p>If you find our work useful, please consider citing our paper:</p>
                <pre><code>@article{lu2025artifacts,
      title={Artifacts and Attention Sinks: Structured Approximations for Efficient Vision Transformers},
      author={Lu, Andrew and Liao, Wentinn and Wang, Liuhui and Yang, Huzheng and Shi, Jianbo},
      journal={arXiv preprint arXiv:2507.16018},
      year={2025},
      url={https://arxiv.org/abs/2507.16018}
}</code></pre>
            </section>
        </main>
    </div>

</body>
</html>